#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SoulByte - æ•°å­—äººç”Ÿæˆç³»åˆ—é¡¹ç›® Â· å›å¤æ¨¡å¼åˆ†æç»„ä»¶
SoulByte - Digital Human Generation Series Â· Reply Pattern Analysis Component

æ­¤æ¨¡å—è´Ÿè´£åˆ†æç”¨æˆ·çš„èŠå¤©è®°å½•ï¼Œæ€»ç»“ç”¨æˆ·çš„å›å¤é€»è¾‘ã€è¯­æ°”ã€è¯­è°ƒå’Œè¡¨è¾¾é£æ ¼ï¼Œ
é€šè¿‡åˆ†æ®µåˆ†æå’Œå¤šå±‚æ¬¡åˆå¹¶çš„æ–¹å¼ï¼Œç”Ÿæˆå…¨é¢çš„ç”¨æˆ·å›å¤ç­–ç•¥æ¦‚è¿°ã€‚
"""

import json
import os
import time
import hashlib
from typing import Dict, List, Any, Tuple
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from managers.config_manager import ConfigManager
from managers.contact_manager import ContactManager
import pandas as pd


class PatternAnalysisCache:
    """å›å¤æ¨¡å¼åˆ†æç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, output_dir: str):
        self.cache_dir = os.path.join(output_dir, "pattern_analysis_cache")
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_file = os.path.join(self.cache_dir, "analysis_cache.json")
        self.api_log_file = os.path.join(self.cache_dir, "api_interactions.json")
        self.cache_data = self._load_cache()
        self.api_interactions = self._load_api_log()
    
    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½åˆ†æç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _load_api_log(self) -> List:
        """åŠ è½½APIäº¤äº’æ—¥å¿—"""
        if os.path.exists(self.api_log_file):
            try:
                with open(self.api_log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"åŠ è½½APIæ—¥å¿—å¤±è´¥: {e}")
        return []
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {e}")
    
    def _save_api_log(self):
        """ä¿å­˜APIäº¤äº’æ—¥å¿—"""
        try:
            with open(self.api_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.api_interactions, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜APIæ—¥å¿—å¤±è´¥: {e}")
    
    def get_cache_key(self, data: Any) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        data_str = json.dumps(data, ensure_ascii=False, sort_keys=True)
        return hashlib.md5(data_str.encode('utf-8')).hexdigest()
    
    def get_cached_result(self, cache_key: str) -> Any:
        """è·å–ç¼“å­˜ç»“æœ"""
        return self.cache_data.get(cache_key)
    
    def save_result(self, cache_key: str, result: Any):
        """ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜"""
        self.cache_data[cache_key] = {
            "result": result,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "cache_key": cache_key
        }
        self._save_cache()
    
    def log_api_interaction(self, prompt: str, response: str, analysis_type: str, 
                           contact_name: str = "", chunk_idx: int = -1):
        """è®°å½•APIäº¤äº’"""
        interaction = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "analysis_type": analysis_type,
            "contact_name": contact_name,
            "chunk_idx": chunk_idx,
            "prompt": prompt,
            "response": response,
            "prompt_length": len(prompt),
            "response_length": len(response)
        }
        self.api_interactions.append(interaction)
        self._save_api_log()
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_cached_results": len(self.cache_data),
            "total_api_interactions": len(self.api_interactions),
            "cache_file": self.cache_file,
            "api_log_file": self.api_log_file
        }


class PatternAnalyzer:
    """å›å¤æ¨¡å¼åˆ†æå™¨ - ä½¿ç”¨å¤§æ¨¡å‹åˆ†æç”¨æˆ·çš„å›å¤æ¨¡å¼å’Œé£æ ¼"""
    
    def __init__(self, config: ConfigManager, output_dir: str = "output"):
        """åˆå§‹åŒ–å›å¤æ¨¡å¼åˆ†æå™¨"""
        self.config = config
        self.output_dir = output_dir
        
        # APIé…ç½®
        self.api_url = config.get('pattern_analysis.api_url')
        self.api_key = config.get('pattern_analysis.api_key')
        self.model = config.get('pattern_analysis.model')
        self.timeout = config.get('pattern_analysis.timeout', 60)
        self.retry_attempts = config.get('pattern_analysis.retry_attempts', 3)
        
        # åˆ†æé…ç½®
        self.chunk_size = config.get('pattern_analysis.chunk_size', 1000)
        self.max_workers = config.get('pattern_analysis.max_workers', 3)
        self.debug_mode = config.get('pattern_analysis.debug_mode', False)
        
        # æç¤ºè¯æ¨¡æ¿
        self.first_level_prompt = config.get('pattern_analysis.first_level_prompt')
        self.second_level_prompt = config.get('pattern_analysis.second_level_prompt')
        self.final_level_prompt = config.get('pattern_analysis.final_level_prompt')
        
        # è”ç³»äººç®¡ç†å™¨å’Œç¼“å­˜ç®¡ç†å™¨
        self.contact_manager = ContactManager(output_dir)
        self.cache = PatternAnalysisCache(output_dir)
        
        # è·å–ç”¨æˆ·è‡ªå·±çš„å¾®ä¿¡ID
        self.my_wxid = config.get('data_processing.my_wxid')
    
    def analyze_chat_patterns(self, messages: List[Dict]) -> Dict:
        """åˆ†æèŠå¤©è®°å½•ä¸­çš„å›å¤æ¨¡å¼
        
        Args:
            messages: æ‰€æœ‰èŠå¤©è®°å½•
        
        Returns:
            åŒ…å«å›å¤æ¨¡å¼åˆ†æç»“æœçš„å­—å…¸
        """
        print("\nğŸ§  === å¼€å§‹åˆ†æå›å¤æ¨¡å¼ ===")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        cache_stats = self.cache.get_cache_stats()
        print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {cache_stats['total_cached_results']} ä¸ªç¼“å­˜ç»“æœ, {cache_stats['total_api_interactions']} æ¬¡APIè°ƒç”¨")
        
        # 1. æŒ‰è”ç³»äººåˆ†ç»„æ¶ˆæ¯
        print("ğŸ‘¥ æŒ‰è”ç³»äººåˆ†ç»„æ¶ˆæ¯...")
        contact_messages = self._group_messages_by_contact(messages)
        
        # 2. ç¬¬ä¸€å±‚åˆ†æï¼šæŒ‰è”ç³»äººå’Œå—åˆ†æ
        print("ğŸ” ç¬¬ä¸€å±‚åˆ†æï¼šæŒ‰è”ç³»äººå’Œå—åˆ†æ...")
        first_level_results = self._perform_first_level_analysis(contact_messages)
        
        # 3. ç¬¬äºŒå±‚åˆ†æï¼šåˆå¹¶æ¯ä¸ªè”ç³»äººçš„ç»“æœ
        print("ğŸ”„ ç¬¬äºŒå±‚åˆ†æï¼šåˆå¹¶æ¯ä¸ªè”ç³»äººçš„ç»“æœ...")
        second_level_results = self._perform_second_level_analysis(first_level_results)
        
        # 4. æœ€ç»ˆåˆ†æï¼šæ•´åˆæ‰€æœ‰è”ç³»äººçš„ç»“æœ
        print("âœ¨ æœ€ç»ˆåˆ†æï¼šæ•´åˆæ‰€æœ‰è”ç³»äººçš„ç»“æœ...")
        final_result = self._perform_final_analysis(second_level_results)
        
        # 5. ä¿å­˜ç»“æœ
        self._save_analysis_results(final_result, first_level_results, second_level_results)
        
        # æ˜¾ç¤ºæœ€ç»ˆç¼“å­˜ç»Ÿè®¡
        final_cache_stats = self.cache.get_cache_stats()
        print(f"ğŸ“Š æœ€ç»ˆç¼“å­˜ç»Ÿè®¡: {final_cache_stats['total_cached_results']} ä¸ªç¼“å­˜ç»“æœ, {final_cache_stats['total_api_interactions']} æ¬¡APIè°ƒç”¨")
        
        return final_result
    
    def _group_messages_by_contact(self, messages: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰è”ç³»äººåˆ†ç»„æ¶ˆæ¯ï¼ŒåŒ…å«åŒå‘å¯¹è¯"""
        print("ğŸ‘¥ æŒ‰è”ç³»äººåˆ†ç»„æ¶ˆæ¯...")
        
        # ç»Ÿè®¡æ¯ä¸ªè”ç³»äººçš„æ¶ˆæ¯æ•°é‡ï¼Œæ‰¾å‡ºç”¨æˆ·è‡ªå·±çš„ID
        talker_counts = {}
        for msg in messages:
            talker = msg['talker']
            talker_counts[talker] = talker_counts.get(talker, 0) + 1
        
        # æ¶ˆæ¯æ•°é‡æœ€å¤šçš„é€šå¸¸æ˜¯ç”¨æˆ·è‡ªå·±
        if talker_counts:
            my_contact_id = max(talker_counts.items(), key=lambda x: x[1])[0]
            print(f"ğŸ” è¯†åˆ«ç”¨æˆ·è‡ªå·±çš„è”ç³»äººID: {my_contact_id} (æ¶ˆæ¯æ•°: {talker_counts[my_contact_id]})")
        
        # ä¼˜åŒ–ï¼šæŒ‰æ—¶é—´æ’åºæ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶å»ºç«‹æ—¶é—´ç´¢å¼•
        print("â° æŒ‰æ—¶é—´æ’åºæ¶ˆæ¯å¹¶å»ºç«‹ç´¢å¼•...")
        sorted_messages = sorted(messages, key=lambda x: pd.to_datetime(x['create_time']) if isinstance(x['create_time'], str) else x['create_time'])
        
        # å»ºç«‹è”ç³»äººæ¶ˆæ¯çš„æ—¶é—´ç´¢å¼•ï¼ˆé™¤äº†ç”¨æˆ·è‡ªå·±ï¼‰
        contact_time_index = {}
        user_messages = []
        
        for i, msg in enumerate(sorted_messages):
            msg_time = pd.to_datetime(msg['create_time']) if isinstance(msg['create_time'], str) else msg['create_time']
            talker = msg['talker']
            
            if talker == my_contact_id:
                user_messages.append((i, msg, msg_time))
            else:
                if talker not in contact_time_index:
                    contact_time_index[talker] = []
                contact_time_index[talker].append((i, msg, msg_time))
        
        print(f"ğŸ“Š ç”¨æˆ·æ¶ˆæ¯: {len(user_messages)} æ¡")
        print(f"ğŸ‘¥ å…¶ä»–è”ç³»äºº: {len(contact_time_index)} ä¸ª")
        
        # ä¼˜åŒ–çš„å¯¹è¯åˆ†ç»„ç®—æ³•
        contact_conversations = {}
        time_window_seconds = 3600  # 1å°æ—¶çª—å£
        
        print("ğŸ” å¼€å§‹æ™ºèƒ½å¯¹è¯åˆ†ç»„...")
        for contact_id, contact_msgs in contact_time_index.items():
            if len(contact_msgs) < 5:  # è·³è¿‡æ¶ˆæ¯å¤ªå°‘çš„è”ç³»äºº
                continue
                
            conversation = []
            contact_name = self.contact_manager.get_display_name(contact_id)
            
            # æ·»åŠ è¯¥è”ç³»äººçš„æ‰€æœ‰æ¶ˆæ¯
            for _, msg, _ in contact_msgs:
                conversation.append(msg)
            
            # ä½¿ç”¨äºŒåˆ†æœç´¢æ‰¾åˆ°ç›¸å…³çš„ç”¨æˆ·æ¶ˆæ¯
            contact_times = [msg_time for _, _, msg_time in contact_msgs]
            min_contact_time = min(contact_times)
            max_contact_time = max(contact_times)
            
            # æ‰©å±•æ—¶é—´çª—å£
            search_start = min_contact_time - pd.Timedelta(seconds=time_window_seconds)
            search_end = max_contact_time + pd.Timedelta(seconds=time_window_seconds)
            
            # æ‰¾åˆ°æ—¶é—´çª—å£å†…çš„ç”¨æˆ·æ¶ˆæ¯
            related_user_msgs = []
            for _, user_msg, user_time in user_messages:
                if search_start <= user_time <= search_end:
                    # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½•è”ç³»äººæ¶ˆæ¯çš„æ—¶é—´çª—å£å†…
                    for _, _, contact_time in contact_msgs:
                        if abs((user_time - contact_time).total_seconds()) <= time_window_seconds:
                            related_user_msgs.append(user_msg)
                            break
            
            # åˆå¹¶å¹¶æŒ‰æ—¶é—´æ’åº
            conversation.extend(related_user_msgs)
            conversation.sort(key=lambda x: pd.to_datetime(x['create_time']) if isinstance(x['create_time'], str) else x['create_time'])
            
            if len(conversation) >= 20:  # åªä¿ç•™æœ‰è¶³å¤Ÿå¯¹è¯çš„è”ç³»äºº
                contact_conversations[contact_id] = conversation
                user_msg_count = sum(1 for msg in conversation if msg['talker'] == my_contact_id)
                other_msg_count = len(conversation) - user_msg_count
                print(f"ğŸ‘¥ è”ç³»äºº '{contact_name}': æ€»è®¡ {len(conversation)} æ¡æ¶ˆæ¯ (ç”¨æˆ·: {user_msg_count}, å¯¹æ–¹: {other_msg_count})")
        
        print(f"âœ… æ‰¾åˆ° {len(contact_conversations)} ä¸ªæœ‰æ•ˆè”ç³»äººï¼ˆæ¶ˆæ¯æ•° >= 20ï¼ŒåŒ…å«åŒå‘å¯¹è¯ï¼‰")
        return contact_conversations
    
    def _perform_first_level_analysis(self, contact_messages: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """ç¬¬ä¸€å±‚åˆ†æï¼šæŒ‰è”ç³»äººå’Œå—åˆ†æ"""
        first_level_results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for contact_id, messages in contact_messages.items():
                contact_name = self.contact_manager.get_display_name(contact_id)
                relationship = self.contact_manager.get_relationship(contact_id)
                
                # æŒ‰å—åˆ†å‰²æ¶ˆæ¯
                message_chunks = [
                    messages[i:i+self.chunk_size] 
                    for i in range(0, len(messages), self.chunk_size)
                ]
                
                print(f"ğŸ“Š è”ç³»äºº '{contact_name}' ({relationship}) å…±æœ‰ {len(messages)} æ¡æ¶ˆæ¯ï¼Œåˆ†ä¸º {len(message_chunks)} ä¸ªå—")
                
                for chunk_idx, chunk in enumerate(message_chunks):
                    futures.append(
                        executor.submit(
                            self._analyze_message_chunk, 
                            contact_id, 
                            contact_name,
                            relationship,
                            chunk, 
                            chunk_idx
                        )
                    )
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(futures, desc="åˆ†ææ¶ˆæ¯å—"):
                result = future.result()
                if result:
                    contact_id = result["contact_id"]
                    if contact_id not in first_level_results:
                        first_level_results[contact_id] = []
                    first_level_results[contact_id].append(result)
        
        return first_level_results
    
    def _analyze_message_chunk(self, contact_id: str, contact_name: str, 
                              relationship: str, messages: List[Dict], 
                              chunk_idx: int) -> Dict:
        """åˆ†æå•ä¸ªæ¶ˆæ¯å—"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_data = {
            "contact_id": contact_id,
            "contact_name": contact_name,
            "relationship": relationship,
            "chunk_idx": chunk_idx,
            "messages": [{"is_sender": msg['is_sender'], "content": msg['content']} for msg in messages]
        }
        cache_key = self.cache.get_cache_key(cache_data)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = self.cache.get_cached_result(cache_key)
        if cached_result:
            if self.debug_mode:
                print(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {contact_name} å— {chunk_idx}")
            return cached_result["result"]
        
        # æå–å¯¹è¯
        conversations = []
        for msg in messages:
            sender = "æˆ‘" if msg['is_sender'] == 1 else contact_name
            conversations.append({
                "sender": sender,
                "content": msg['content'],
                "create_time": msg['create_time']
            })
        
        # æ„å»ºæç¤ºè¯
        prompt = self.first_level_prompt.format(
            contact_name=contact_name,
            relationship=relationship,
            conversation_count=len(messages),
            conversations=json.dumps(conversations, ensure_ascii=False)
        )
        
        # è°ƒç”¨API
        try:
            if self.debug_mode:
                print(f"ğŸ” åˆ†ææ¶ˆæ¯å—: {contact_name} å— {chunk_idx}")
                print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            analysis = self._call_api(prompt)
            
            # è®°å½•APIäº¤äº’
            self.cache.log_api_interaction(
                prompt=prompt,
                response=analysis,
                analysis_type="first_level",
                contact_name=contact_name,
                chunk_idx=chunk_idx
            )
            
            if self.debug_mode:
                print(f"âœ… åˆ†æå®Œæˆ: {contact_name} å— {chunk_idx}")
                print(f"ğŸ“„ å›å¤é•¿åº¦: {len(analysis)} å­—ç¬¦")
                print(f"ğŸ” å›å¤å†…å®¹é¢„è§ˆ: {analysis[:200]}...")
            
            result = {
                "contact_id": contact_id,
                "contact_name": contact_name,
                "relationship": relationship,
                "chunk_idx": chunk_idx,
                "message_count": len(messages),
                "analysis": analysis
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache.save_result(cache_key, result)
            
            return result
        except Exception as e:
            print(f"âŒ åˆ†æå—å¤±è´¥ (è”ç³»äºº: {contact_name}, å—: {chunk_idx}): {e}")
            return None
    
    def _perform_second_level_analysis(self, first_level_results: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """ç¬¬äºŒå±‚åˆ†æï¼šåˆå¹¶æ¯ä¸ªè”ç³»äººçš„ç»“æœ"""
        second_level_results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for contact_id, analyses in first_level_results.items():
                if not analyses:
                    continue
                
                contact_name = analyses[0]["contact_name"]
                relationship = analyses[0]["relationship"]
                
                futures.append(
                    executor.submit(
                        self._merge_contact_analyses,
                        contact_id,
                        contact_name,
                        relationship,
                        analyses
                    )
                )
            
            # æ”¶é›†ç»“æœ
            for future in tqdm(futures, desc="åˆå¹¶è”ç³»äººåˆ†æ"):
                result = future.result()
                if result:
                    second_level_results[result["contact_id"]] = result
        
        return second_level_results
    
    def _merge_contact_analyses(self, contact_id: str, contact_name: str, 
                               relationship: str, analyses: List[Dict]) -> Dict:
        """åˆå¹¶å•ä¸ªè”ç³»äººçš„æ‰€æœ‰åˆ†æç»“æœ"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_data = {
            "contact_id": contact_id,
            "contact_name": contact_name,
            "relationship": relationship,
            "analyses": [analysis["analysis"] for analysis in analyses]
        }
        cache_key = self.cache.get_cache_key(cache_data)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = self.cache.get_cached_result(cache_key)
        if cached_result:
            if self.debug_mode:
                print(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: {contact_name} äºŒçº§åˆ†æ")
            return cached_result["result"]
        
        # æå–æ‰€æœ‰åˆ†æç»“æœ
        all_analyses = [analysis["analysis"] for analysis in analyses]
        total_messages = sum(analysis["message_count"] for analysis in analyses)
        
        # æ„å»ºæç¤ºè¯
        prompt = self.second_level_prompt.format(
            contact_name=contact_name,
            relationship=relationship,
            analysis_count=len(analyses),
            total_messages=total_messages,
            analyses=json.dumps(all_analyses, ensure_ascii=False)
        )
        
        # è°ƒç”¨API
        try:
            if self.debug_mode:
                print(f"ğŸ”„ åˆå¹¶è”ç³»äººåˆ†æ: {contact_name}")
                print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            merged_analysis = self._call_api(prompt)
            
            # è®°å½•APIäº¤äº’
            self.cache.log_api_interaction(
                prompt=prompt,
                response=merged_analysis,
                analysis_type="second_level",
                contact_name=contact_name
            )
            
            if self.debug_mode:
                print(f"âœ… åˆå¹¶å®Œæˆ: {contact_name}")
                print(f"ğŸ“„ å›å¤é•¿åº¦: {len(merged_analysis)} å­—ç¬¦")
                print(f"ğŸ” å›å¤å†…å®¹é¢„è§ˆ: {merged_analysis[:200]}...")
            
            result = {
                "contact_id": contact_id,
                "contact_name": contact_name,
                "relationship": relationship,
                "analysis_count": len(analyses),
                "total_messages": total_messages,
                "merged_analysis": merged_analysis
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache.save_result(cache_key, result)
            
            return result
        except Exception as e:
            print(f"âŒ åˆå¹¶è”ç³»äººåˆ†æå¤±è´¥ (è”ç³»äºº: {contact_name}): {e}")
            return None
    
    def _perform_final_analysis(self, second_level_results: Dict[str, Dict]) -> Dict:
        """æœ€ç»ˆåˆ†æï¼šæ•´åˆæ‰€æœ‰è”ç³»äººçš„ç»“æœ"""
        # ç”Ÿæˆç¼“å­˜é”®
        contact_analyses = []
        for contact_id, result in second_level_results.items():
            contact_analyses.append({
                "contact_name": result["contact_name"],
                "relationship": result["relationship"],
                "total_messages": result["total_messages"],
                "analysis": result["merged_analysis"]
            })
        
        cache_key = self.cache.get_cache_key(contact_analyses)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = self.cache.get_cached_result(cache_key)
        if cached_result:
            if self.debug_mode:
                print("ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ: æœ€ç»ˆåˆ†æ")
            return cached_result["result"]
        
        total_contacts = len(second_level_results)
        total_messages = sum(result["total_messages"] for result in second_level_results.values())
        
        # æ„å»ºæç¤ºè¯
        prompt = self.final_level_prompt.format(
            contact_count=total_contacts,
            total_messages=total_messages,
            contact_analyses=json.dumps(contact_analyses, ensure_ascii=False)
        )
        
        # è°ƒç”¨API
        try:
            if self.debug_mode:
                print("âœ¨ æ‰§è¡Œæœ€ç»ˆåˆ†æ")
                print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            final_analysis = self._call_api(prompt)
            
            # è®°å½•APIäº¤äº’
            self.cache.log_api_interaction(
                prompt=prompt,
                response=final_analysis,
                analysis_type="final_level"
            )
            
            if self.debug_mode:
                print("âœ… æœ€ç»ˆåˆ†æå®Œæˆ")
                print(f"ğŸ“„ å›å¤é•¿åº¦: {len(final_analysis)} å­—ç¬¦")
                print(f"ğŸ” å›å¤å†…å®¹é¢„è§ˆ: {final_analysis[:500]}...")
            
            result = {
                "total_contacts": total_contacts,
                "total_messages": total_messages,
                "final_analysis": final_analysis,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache.save_result(cache_key, result)
            
            return result
        except Exception as e:
            print(f"âŒ æœ€ç»ˆåˆ†æå¤±è´¥: {e}")
            return {
                "total_contacts": total_contacts,
                "total_messages": total_messages,
                "final_analysis": f"åˆ†æå¤±è´¥: {e}",
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            }
    
    def _call_api(self, prompt: str) -> str:
        """è°ƒç”¨å¤§æ¨¡å‹API"""
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': self.model,
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'max_tokens': 10000,
            'temperature': 0.7,
            'top_p': 0.95,
        }
        
        for attempt in range(self.retry_attempts):
            try:
                response = requests.post(
                    self.api_url,
                    headers=headers,
                    json=data,
                    timeout=self.timeout
                )
                
                response.raise_for_status()
                result = response.json()
                
                return result['choices'][0]['message']['content'].strip()
            except Exception as e:
                if attempt == self.retry_attempts - 1:
                    raise e
                time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
        
        return "APIè°ƒç”¨å¤±è´¥"
    
    def _save_analysis_results(self, final_result: Dict, 
                              first_level_results: Dict[str, List[Dict]],
                              second_level_results: Dict[str, Dict]) -> None:
        """ä¿å­˜åˆ†æç»“æœ"""
        # åˆ›å»ºç»“æœç›®å½•
        analysis_dir = os.path.join(self.output_dir, "pattern_analysis")
        os.makedirs(analysis_dir, exist_ok=True)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_file = os.path.join(analysis_dir, "final_analysis.json")
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç¬¬ä¸€å±‚ç»“æœ
        first_level_file = os.path.join(analysis_dir, "first_level_analysis.json")
        with open(first_level_file, 'w', encoding='utf-8') as f:
            json.dump(first_level_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç¬¬äºŒå±‚ç»“æœ
        second_level_file = os.path.join(analysis_dir, "second_level_analysis.json")
        with open(second_level_file, 'w', encoding='utf-8') as f:
            json.dump(second_level_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        cache_stats_file = os.path.join(analysis_dir, "cache_statistics.json")
        cache_stats = self.cache.get_cache_stats()
        with open(cache_stats_file, 'w', encoding='utf-8') as f:
            json.dump(cache_stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_dir}")
        print(f"ğŸ“Š æœ€ç»ˆåˆ†æ: {final_file}")
        print(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {cache_stats_file}")
        print(f"ğŸ“ APIäº¤äº’æ—¥å¿—: {cache_stats['api_log_file']}") 